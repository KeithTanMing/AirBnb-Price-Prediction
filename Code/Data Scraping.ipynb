{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:12.406863Z",
     "start_time": "2019-12-03T15:10:11.570069Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:19.670333Z",
     "start_time": "2019-12-03T15:10:12.409721Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL where data is obtained, and its output folder\n",
    "URL = 'http://insideairbnb.com/get-the-data.html'\n",
    "OUTPUT_DIR = '../Asset'\n",
    "\n",
    "u = urlopen(URL)\n",
    "try:\n",
    "    html = u.read().decode('utf-8')\n",
    "finally:\n",
    "    u.close()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "date = []\n",
    "href = []\n",
    "\n",
    "# Filter html to find first td tag in each table class, to save datestamp of file\n",
    "for table in soup.findAll(\"table\", {\"class\": \"table table-hover table-striped new-york-city\"}):\n",
    "    for tr in table.findAll('tr'):\n",
    "        tdTag = tr.find('td')\n",
    "        try:\n",
    "            date.append(tdTag.text) # Save date\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    for a in table.findAll('a'):\n",
    "        href.append(a['href']) # Save file download link\n",
    "              \n",
    "# Save into dataframe\n",
    "href_df = pd.DataFrame(list(zip(date, href)), columns =['Date', 'Href'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:19.678155Z",
     "start_time": "2019-12-03T15:10:19.672897Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To replace missing date values with preceding date values\n",
    "href_df.Date = href_df.Date.replace({'N/A': np.nan}).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:19.692970Z",
     "start_time": "2019-12-03T15:10:19.681007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "href_df['Date'] = pd.to_datetime(href_df['Date'])\n",
    "\n",
    "# Set date column as index\n",
    "href_df.set_index(['Date']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:19.700202Z",
     "start_time": "2019-12-03T15:10:19.694657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create YearMonth column for file nomenclature later\n",
    "href_df['YearMonth'] = href_df['Date'].map(lambda x: x.month + 100*x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:19.707331Z",
     "start_time": "2019-12-03T15:10:19.703109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create file column in dictionary for file type\n",
    "file = []\n",
    "for i in href_df.Href:\n",
    "    file.append(i.split('/')[-1])\n",
    "    \n",
    "href_df['file'] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:19.718992Z",
     "start_time": "2019-12-03T15:10:19.709175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove unecessary file downloads link from dataframe\n",
    "href_df = href_df.drop(href_df[href_df.file == 'listings.csv'].index)\n",
    "href_df = href_df.drop(href_df[href_df.file == 'reviews.csv'].index)\n",
    "href_df = href_df.drop(href_df[href_df.file == 'neighbourhoods.csv'].index)\n",
    "href_df.reset_index(inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:21.246004Z",
     "start_time": "2019-12-03T15:10:21.242315Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter for 1 year (to be downloaded)\n",
    "href_1yr = href_df.head(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:21.456794Z",
     "start_time": "2019-12-03T15:10:21.437977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Href</th>\n",
       "      <th>YearMonth</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/data/listings.csv.gz</td>\n",
       "      <td>201909</td>\n",
       "      <td>listings.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/data/calendar.csv.gz</td>\n",
       "      <td>201909</td>\n",
       "      <td>calendar.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/data/reviews.csv.gz</td>\n",
       "      <td>201909</td>\n",
       "      <td>reviews.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/visualisations/neighbourhoods.geojson</td>\n",
       "      <td>201909</td>\n",
       "      <td>neighbourhoods.geojson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/data/listings.csv.gz</td>\n",
       "      <td>201908</td>\n",
       "      <td>listings.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/data/calendar.csv.gz</td>\n",
       "      <td>201908</td>\n",
       "      <td>calendar.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/data/reviews.csv.gz</td>\n",
       "      <td>201908</td>\n",
       "      <td>reviews.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/visualisations/neighbourhoods.geojson</td>\n",
       "      <td>201908</td>\n",
       "      <td>neighbourhoods.geojson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       Date  \\\n",
       "0  0     2019-09-12   \n",
       "1  1     2019-09-12   \n",
       "2  2     2019-09-12   \n",
       "3  6     2019-09-12   \n",
       "4  7     2019-08-06   \n",
       "5  8     2019-08-06   \n",
       "6  9     2019-08-06   \n",
       "7  13    2019-08-06   \n",
       "\n",
       "                                                                                                           Href  \\\n",
       "0  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/data/listings.csv.gz                    \n",
       "1  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/data/calendar.csv.gz                    \n",
       "2  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/data/reviews.csv.gz                     \n",
       "3  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-09-12/visualisations/neighbourhoods.geojson   \n",
       "4  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/data/listings.csv.gz                    \n",
       "5  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/data/calendar.csv.gz                    \n",
       "6  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/data/reviews.csv.gz                     \n",
       "7  http://data.insideairbnb.com/united-states/ny/new-york-city/2019-08-06/visualisations/neighbourhoods.geojson   \n",
       "\n",
       "   YearMonth                    file  \n",
       "0  201909     listings.csv.gz         \n",
       "1  201909     calendar.csv.gz         \n",
       "2  201909     reviews.csv.gz          \n",
       "3  201909     neighbourhoods.geojson  \n",
       "4  201908     listings.csv.gz         \n",
       "5  201908     calendar.csv.gz         \n",
       "6  201908     reviews.csv.gz          \n",
       "7  201908     neighbourhoods.geojson  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "href_1yr.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is collected in the middle of each month. For each month, over a period of 1 year, 4 files will be downloaded:\n",
    "<br>1) Full listing details of current listings - listings.csv.gz\n",
    "<br>2) 1 year forward calendar availability     - calendar.csv.gz\n",
    "<br>3) Reviews till date for each listing       - reviews.csv.gz\n",
    "<br>4) Geojson file for the listings            - neighbourhoods.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:10:36.422579Z",
     "start_time": "2019-12-03T15:10:36.420254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Downloading of latest 12 months files\n",
    "for num, link in enumerate(href_1yr['Href']):\n",
    "    filename = os.path.join(OUTPUT_DIR, str(href_1yr.YearMonth[num]) + '_' + str(href_1yr.file[num]))\n",
    "\n",
    "    print(\"Downloading %s to %s...\" % (link, filename) )\n",
    "    urlretrieve(link, filename)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:11:05.931805Z",
     "start_time": "2019-12-03T15:11:05.929088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a merged listing dataframe across the 12 months, and removing duplicates\n",
    "listings = pd.DataFrame()\n",
    "\n",
    "for i in href_1yr['YearMonth'].unique().tolist():\n",
    "    listed = pd.read_csv('../Asset/{}'.format(str(i) + '_' + 'listings.csv.gz'), compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "    listings = pd.concat([listings,listed], ignore_index=True, sort=False).drop_duplicates(subset='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:11:39.827281Z",
     "start_time": "2019-12-03T15:11:39.824703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a merged reviews dataframe across the 12 months, and removing duplicates\n",
    "reviews = pd.DataFrame()\n",
    "\n",
    "for i in href_1yr['YearMonth'].unique().tolist():\n",
    "    rev = pd.read_csv('../Asset/{}'.format(str(i) + '_' + 'reviews.csv.gz'), compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "    reviews = pd.concat([reviews,rev], ignore_index=True, sort=False).drop_duplicates(subset='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:11:49.053278Z",
     "start_time": "2019-12-03T15:11:49.050342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export out merged files for modelling in separate notebook\n",
    "listings.to_csv(r'../Asset/listing_merged.csv', index=False)\n",
    "reviews.to_csv(r'../Asset/reviews_merged.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
